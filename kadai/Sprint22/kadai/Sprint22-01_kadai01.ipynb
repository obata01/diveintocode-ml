{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled0.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"authorship_tag":"ABX9TyOFqgepXOiBFpwWhoCXGoNe"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"E1JhzFwft6JD"},"source":["# Sprint 深層学習スクラッチ リカレントニューラルネットワーク"]},{"cell_type":"markdown","metadata":{"id":"iUgNpYrst6mW"},"source":["## 1.このSprintについて\n","\n","### Sprintの目的\n","- スクラッチを通してリカレントニューラルネットワークの基礎を理解する\n","\n","### どのように学ぶか\n","- スクラッチでリカレントニューラルネットワークの実装を行います。"]},{"cell_type":"markdown","metadata":{"id":"r1riFl1St6pn"},"source":["## 2.リカレントニューラルネットワークスクラッチ\n","\n","リカレントニューラルネットワーク（RNN） のクラスをスクラッチで作成していきます。NumPyなど最低限のライブラリのみを使いアルゴリズムを実装していきます。\n","\n","\n","フォワードプロパゲーションの実装を必須課題とし、バックプロパゲーションの実装はアドバンス課題とします。\n","\n","\n","クラスの名前はScratchSimpleRNNClassifierとしてください。クラスの構造などは以前のSprintで作成したScratchDeepNeuralNetrowkClassifierを参考にしてください。"]},{"cell_type":"markdown","metadata":{"id":"aDO8gua6vwVz"},"source":["## 【まとめ】最終コード"]},{"cell_type":"code","metadata":{"id":"dtia2SNBKxhp"},"source":["import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d-KanSXdAI9X"},"source":["### ■ ScratchSimpleRNNClassifier:"]},{"cell_type":"code","metadata":{"id":"ume0BLS2vvW6"},"source":["class ScratchSimpleRNNClassifier:\n","    def __init__(self):\n","        pass\n","\n","\n","    def fit(self, X, sigma=0.01, lr=0.01, epochs=1):\n","        \"\"\"\n","        X : (batch_size, n_sequences, n_features)\n","        out : (batch_size, n_sequences, n_nodes)\n","        \"\"\"\n","        self.sigma = sigma\n","        self.lr = lr\n","        self.epochs = epochs\n","        \n","        self.n_samples = X.shape[0]\n","        self.n_sequences = X.shape[1]\n","        self.n_features = X.shape[2]\n","        self.n_nodes = 4   # 任意値\n","\n","        self.activation = Tanh()\n","        self.rnn = MyRNN(input_seq=self.n_sequences,\n","                         input_features=self.n_features,\n","                         initializer=HeInitializer(self.sigma, self.n_features, self.n_nodes),\n","                         optimizer=SGD(lr=self.lr),\n","                         activation=self.activation)\n","\n","        # 本来はミニバッチ毎に実行\n","        out = self.forward(X)\n","        return out\n","\n","\n","    def forward(self, inputs):\n","        x = self.rnn.forward(inputs)\n","        return x\n","\n","\n","    def backward(self, dA):\n","        dZ = self.rnn.backward(dA)\n","        return dZ\n","\n","\n","    def predict(self):\n","        pass\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1eRsDyzhAOh5"},"source":["### ■ MyRNN"]},{"cell_type":"code","metadata":{"id":"xS8v1Ds3zo0v","executionInfo":{"status":"ok","timestamp":1602058606463,"user_tz":-540,"elapsed":719,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}}},"source":["class MyRNN():\n","    def __init__(self, input_seq, input_features, initializer, optimizer, activation):\n","        self.optimizer = optimizer\n","        self.activation = activation\n","        # w,bの初期化\n","        self.Wx = initializer.W()\n","        self.B = initializer.B()\n","\n","        # 各サイズ\n","        self.batch_size = None\n","        self.n_sequences = input_seq\n","        self.n_features = input_features\n","        self.n_nodes = self.Wx.shape[-1]\n","\n","        # 隠れ状態ベクトルの重み\n","        self.Wh = initializer.W(self.n_nodes, self.n_nodes)\n","\n","        # AdaGrad用\n","        self.H_W = np.zeros([self.Wx.shape[0], self.Wx.shape[1]])\n","        self.H_B = np.zeros(self.B.shape[1])\n","\n","        # ※問題2に合わせてパラメータを上書き！！\n","        self.Wx = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n","        self.Wh = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n","        self.n_nodes = self.Wx.shape[1] # 4\n","        # h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n","        self.B = np.array([[1, 1, 1, 1]]).astype('float') # (n_nodes,)\n","\n","\n","    def forward(self, X):\n","        self.X = X\n","        self.batch_size = X.shape[0]\n","        self.S = np.zeros((self.batch_size, self.n_sequences, self.n_nodes))\n","\n","        for t in range(self.n_sequences):\n","            self.S[:, t, :] = self.activation(X[:, t, :]@self.Wx + self.S[:, t-1, :]@self.Wh + self.B)\n","        return self.S\n","\n","\n","    def backward(self, dA):\n","        dHt_1 = np.zeros((self.batch_size, 1, self.n_nodes))\n","        self.dWx = np.zeros((1, self.n_features, self.n_nodes))\n","        self.dWh = np.zeros((1, self.n_nodes, self.n_nodes))\n","        self.dB = np.zeros((1, 1, self.n_nodes))\n","        dXt = np.zeros((self.batch_size, self.n_sequences, self.n_features))\n","\n","        for t in range(self.n_sequences-1, -1, -1):\n","            dHt = dA[:, t, :] + dHt_1    # 前の時刻からの状態の誤差と出力の誤差の合計\n","            print('■ dHt_1 ----------------------------------\\n', dHt_1)\n","            print('■ dHt ----------------------------------\\n', dHt)\n","            print('■ dA ----------------------------------\\n', dA)\n","\n","            dAt = dHt * (1 - np.tanh(dA[:, t, :])**2)\n","            print('■ dAt ----------------------------------\\n', dAt)\n","            \n","            self.dWx += self.X[:, t, :].T @ dAt\n","            print('■ dWx ----------------------------------\\n', self.dWx)\n","            self.dB += dAt\n","\n","            self.dWh += self.S[:, t, :].T @ dAt\n","            print('■ self.dWh ----------------------------------\\n', self.dWh)\n","\n","            dHt_1 = dAt @ self.Wh.T\n","            dXt[:, t:t+1, :] += dAt @ self.Wx.T\n","            print('■ dHt_1 ----------------------------------\\n', dHt_1)\n","            print('■ dXt ----------------------------------\\n', dXt)\n","        \n","        # 蓄積した各パラメータの形状を整える\n","        self.dWx = self.dWx.squeeze()\n","        self.dWh = self.dWh.squeeze()\n","        self.dB = self.dB.squeeze()\n","\n","        # 更新\n","        self.optimizer.rnn_update(self)\n","        return dXt\n"],"execution_count":26,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J1VeT7bmASho"},"source":["### ■ SimpleInitializer"]},{"cell_type":"code","metadata":{"id":"TssaNrMQ3vaR"},"source":["class SimpleInitializer:\n","    def __init__(self, sigma, n_nodes1, n_nodes2):\n","        self.sigma = sigma\n","        self.n_nodes1 = n_nodes1\n","        self.n_nodes2 = n_nodes2\n","        self.mean = 0\n","        self.s = 1\n","\n","    def W(self, n_nodes1=None, n_nodes2=None):\n","        if np.any(n_nodes1 != None):\n","            self.n_nodes1 = n_nodes1\n","        if np.any(n_nodes2 != None):\n","            self.n_nodes2 = n_nodes2\n","\n","        W = self.sigma * np.random.normal(loc=self.mean, scale=self.s, size=(self.n_nodes1, self.n_nodes2))\n","        return W\n","\n","\n","    def B(self, num=None):\n","        if num == None:\n","            # B = self.sigma * np.random.normal(loc=self.mean, scale=self.s, size=(1, self.n_nodes2))\n","            B = np.zeros([1, self.n_nodes2])\n","        else:\n","            B = np.zeros([1, num])\n","        return B\n","\n","class XavierInitializer(SimpleInitializer):\n","    def __init__(self, sigma, n_nodes1, n_nodes2):\n","        super().__init__(sigma, n_nodes1, n_nodes2)\n","        self.sigma = 1\n","        self.s = 1 / np.sqrt(n_nodes1)\n","\n","class HeInitializer(SimpleInitializer):\n","    def __init__(self, sigma, n_nodes1, n_nodes2):\n","        super().__init__(sigma, n_nodes1, n_nodes2)\n","        self.sigma = 1\n","        self.s = np.sqrt(2 / self.n_nodes1)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NdnSDD4ZAV6b"},"source":["### ■ Optimizer"]},{"cell_type":"code","metadata":{"id":"-BWT4KjO3vd5","executionInfo":{"status":"ok","timestamp":1602058603464,"user_tz":-540,"elapsed":653,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}}},"source":["class SGD:\n","    def __init__(self, lr):\n","        self.lr = lr\n","\n","    def rnn_update(self, layer):\n","        layer.Wx -= self.lr * layer.dWx / layer.batch_size\n","        layer.Wh -= self.lr * layer.dWh / layer.batch_size\n","        layer.B -= self.lr * layer.dB / layer.batch_size\n","        return layer.Wx, layer.Wh"],"execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wqTDrNrCAaI0"},"source":["### ■ Activation"]},{"cell_type":"code","metadata":{"id":"JCSLuVvh3vrn"},"source":["class Softmax:\n","    def __init__(self):\n","        self.loss = []\n","        self.val_loss = []\n","        pass\n","\n","    def forward(self, A):\n","        A -= np.max(A)   # オーバーフロー対策\n","        self.Z = np.exp(A) / np.sum(np.exp(A), axis=1).reshape(-1, 1)\n","        return self.Z\n","\n","    def backward(self, Z, Y):\n","        dA = Z - Y\n","        self.cross_entropy_loss(Z, Y)\n","        return dA\n","    \n","    def cross_entropy_loss(self, Z, Y, val=False):\n","        batch_size = Z.shape[0]\n","        delta = 1e-7\n","        loss = - np.sum(Y * np.log(Z + delta)) / batch_size\n","        if not val:\n","            self.loss.append(loss)\n","        else:\n","            self.val_loss.append(loss)\n","        return loss\n","\n","\n","class Tanh:\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, A):\n","        self.Z = np.tanh(A)\n","        return self.Z\n","\n","    def backward(self, dZ):\n","        dA = dZ * (1 - dZ**2)\n","        return dA"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NNT1RVNDt6tO"},"source":["## 【問題1】SimpleRNNのフォワードプロパゲーション実装\n","SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n","\n","\n","フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n","\n","\n","バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n","\n","$$\n","a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + B\\\\\n","h_t = tanh(a_t)\n","$$\n","\n","a\n","t\n"," : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)\n","\n","\n","h\n","t\n"," : 時刻tの状態・出力 (batch_size, n_nodes)\n","\n","\n","x\n","t\n"," : 時刻tの入力 (batch_size, n_features)\n","\n","\n","W\n","x\n"," : 入力に対する重み (n_features, n_nodes)\n","\n","\n","h\n","t\n","−\n","1\n"," : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)\n","\n","\n","W\n","h\n"," : 状態に対する重み。 (n_nodes, n_nodes)\n","\n","\n","B\n"," : バイアス項 (n_nodes,)\n","\n","\n","初期状態 \n","h\n","0\n"," は全て0とすることが多いですが、任意の値を与えることも可能です。\n","\n","\n","上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 \n","x\n"," は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n","\n","\n","分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"]},{"cell_type":"markdown","metadata":{"id":"CVQpuK6Ut6wM"},"source":["## 【問題2】小さな配列でのフォワードプロパゲーションの実験\n","小さな配列でフォワードプロパゲーションを考えてみます。\n","\n","\n","入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n","\n","\n","ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。\n","\n"]},{"cell_type":"markdown","metadata":{"id":"gnlwV3lFLeux"},"source":["```\n","x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n","w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n","w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n","batch_size = x.shape[0] # 1\n","n_sequences = x.shape[1] # 3\n","n_features = x.shape[2] # 2\n","n_nodes = w_x.shape[1] # 4\n","h = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n","b = np.array([1, 1, 1, 1]) # (n_nodes,)\n","```"]},{"cell_type":"markdown","metadata":{"id":"BLLapYSMKusG"},"source":["フォワードプロパゲーションの出力が次のようになることを作成したコードで確認してください。\n","\n","```\n","h = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)\n","```"]},{"cell_type":"markdown","metadata":{"id":"3lrp0x9dK4Wo"},"source":["### RNN実行"]},{"cell_type":"code","metadata":{"id":"simHW10YK3tT","executionInfo":{"status":"ok","timestamp":1602056615332,"user_tz":-540,"elapsed":470,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"dab3ecc4-5d84-4ce1-edb8-ba55b0889fde","colab":{"base_uri":"https://localhost:8080/"}},"source":["x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n","rnn = ScratchSimpleRNNClassifier()\n","rnn.fit(x)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[0.76188798, 0.76213958, 0.76239095, 0.76255841],\n","        [0.792209  , 0.8141834 , 0.83404912, 0.84977719],\n","        [0.79494228, 0.81839002, 0.83939649, 0.85584174]]])"]},"metadata":{"tags":[]},"execution_count":23}]},{"cell_type":"markdown","metadata":{"id":"oqgSqN_Et6zG"},"source":["## 【問題3】（アドバンス課題）バックプロパゲーションの実装\n","バックプロパゲーションを実装してください。\n","\n","\n","RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n","\n","$$\n","W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n","W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n","B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"uWA_5e5Gt62E"},"source":["α\n","  : 学習率\n","\n","\n","∂\n","L\n","∂\n","W\n","x\n"," : \n","W\n","x\n"," に関する損失 \n","L\n"," の勾配\n","\n","\n","∂\n","L\n","∂\n","W\n","h\n"," : \n","W\n","h\n"," に関する損失 \n","L\n"," の勾配\n","\n","\n","∂\n","L\n","∂\n","B\n"," : \n","B\n"," に関する損失 \n","L\n"," の勾配\n","\n","\n","勾配を求めるためのバックプロパゲーションの数式が以下です。\n","\n","\n","∂\n","h\n","t\n","∂\n","a\n","t\n","=\n","∂\n","L\n","∂\n","h\n","t\n","×\n","(\n","1\n","−\n","t\n","a\n","n\n","h\n","2\n","(\n","a\n","t\n",")\n",")\n","\n","∂\n","L\n","∂\n","B\n","=\n","∂\n","h\n","t\n","∂\n","a\n","t\n","\n","∂\n","L\n","∂\n","W\n","x\n","=\n","x\n","T\n","t\n","⋅\n","∂\n","h\n","t\n","∂\n","a\n","t\n","\n","∂\n","L\n","∂\n","W\n","h\n","=\n","h\n","T\n","t\n","−\n","1\n","⋅\n","∂\n","h\n","t\n","∂\n","a\n","t\n","\n","＊\n","∂\n","L\n","∂\n","h\n","t\n"," は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n","\n","\n","前の時刻や層に流す誤差の数式は以下です。\n","\n","\n","∂\n","L\n","∂\n","h\n","t\n","−\n","1\n","=\n","∂\n","h\n","t\n","∂\n","a\n","t\n","⋅\n","W\n","T\n","h\n","\n","∂\n","L\n","∂\n","x\n","t\n","=\n","∂\n","h\n","t\n","∂\n","a\n","t\n","⋅\n","W\n","T\n","x"]},{"cell_type":"code","metadata":{"id":"FwwjDDOYaUg4","executionInfo":{"status":"ok","timestamp":1602058610343,"user_tz":-540,"elapsed":530,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"70e1cb62-7ac5-4b5e-b23b-bfe90b1ea20c","colab":{"base_uri":"https://localhost:8080/"}},"source":["dA = np.array([[[-0.02149633,  0.00632246,  0.01809524, -0.01156441],\n","        [-0.09329432,  0.02335331, -0.00404074, -0.02144627],\n","        [-0.21688441, -0.01943171,  0.02462611, -0.01815572]]])\n","rnn2 = ScratchSimpleRNNClassifier()\n","rnn2.fit(x)\n","rnn2.backward(dA)"],"execution_count":27,"outputs":[{"output_type":"stream","text":["■ dHt_1 ----------------------------------\n"," [[[0. 0. 0. 0.]]]\n","■ dHt ----------------------------------\n"," [[[-0.21688441 -0.01943171  0.02462611 -0.01815572]]]\n","■ dA ----------------------------------\n"," [[[-0.02149633  0.00632246  0.01809524 -0.01156441]\n","  [-0.09329432  0.02335331 -0.00404074 -0.02144627]\n","  [-0.21688441 -0.01943171  0.02462611 -0.01815572]]]\n","■ dAt ----------------------------------\n"," [[[-0.20699402 -0.01942437  0.02461118 -0.01814974]]]\n","■ dWx ----------------------------------\n"," [[[-0.00620982 -0.00058273  0.00073834 -0.00054449]\n","  [-0.00827976 -0.00077697  0.00098445 -0.00072599]]]\n","■ self.dWh ----------------------------------\n"," [[[-0.1645483  -0.01544126  0.01956447 -0.01442799]\n","  [-0.16940184 -0.01589671  0.02014155 -0.01485356]\n","  [-0.17375005 -0.01630475  0.02065854 -0.01523483]\n","  [-0.17715412 -0.01662419  0.02106328 -0.0155333 ]]]\n","■ dHt_1 ----------------------------------\n"," [[[-0.00269259 -0.00489216 -0.00691024 -0.0092913 ]]]\n","■ dXt ----------------------------------\n"," [[[ 0.          0.        ]\n","  [ 0.          0.        ]\n","  [-0.00269259 -0.00691024]]]\n","■ dHt_1 ----------------------------------\n"," [[[-0.00269259 -0.00489216 -0.00691024 -0.0092913 ]]]\n","■ dHt ----------------------------------\n"," [[[-0.09598691  0.01846115 -0.01095098 -0.03073757]]]\n","■ dA ----------------------------------\n"," [[[-0.02149633  0.00632246  0.01809524 -0.01156441]\n","  [-0.09329432  0.02335331 -0.00404074 -0.02144627]\n","  [-0.21688441 -0.01943171  0.02462611 -0.01815572]]]\n","■ dAt ----------------------------------\n"," [[[-0.09515628  0.01845108 -0.0109508  -0.03072344]]]\n","■ dWx ----------------------------------\n"," [[[-0.00811295 -0.00021371  0.00051932 -0.00115896]\n","  [-0.01113445 -0.00022344  0.00065592 -0.00164769]]]\n","■ self.dWh ----------------------------------\n"," [[[-0.23993196 -0.00082414  0.01088915 -0.03876738]\n","  [-0.24687651 -0.00087415  0.01122559 -0.03986808]\n","  [-0.25311507 -0.00091564  0.01152504 -0.04085968]\n","  [-0.25801576 -0.00094488  0.01175754 -0.04164138]]]\n","■ dHt_1 ----------------------------------\n"," [[[-0.00309621 -0.00428001 -0.00515657 -0.00664759]]]\n","■ dXt ----------------------------------\n"," [[[ 0.          0.        ]\n","  [-0.00309621 -0.00515657]\n","  [-0.00269259 -0.00691024]]]\n","■ dHt_1 ----------------------------------\n"," [[[-0.00309621 -0.00428001 -0.00515657 -0.00664759]]]\n","■ dHt ----------------------------------\n"," [[[-0.02459254  0.00204245  0.01293867 -0.018212  ]]]\n","■ dA ----------------------------------\n"," [[[-0.02149633  0.00632246  0.01809524 -0.01156441]\n","  [-0.09329432  0.02335331 -0.00404074 -0.02144627]\n","  [-0.21688441 -0.01943171  0.02462611 -0.01815572]]]\n","■ dAt ----------------------------------\n"," [[[-0.02458118  0.00204237  0.01293444 -0.01820957]]]\n","■ dWx ----------------------------------\n"," [[[-0.00835876 -0.00019329  0.00064866 -0.00134106]\n","  [-0.01162607 -0.0001826   0.00091461 -0.00201188]]]\n","■ self.dWh ----------------------------------\n"," [[[-0.25866007  0.00073192  0.02074374 -0.05264103]\n","  [-0.2656108   0.00068242  0.02108344 -0.05374631]\n","  [-0.27185554  0.00064144  0.02138614 -0.05474249]\n","  [-0.27676035  0.00061255  0.0216208  -0.05552724]]]\n","■ dHt_1 ----------------------------------\n"," [[[-0.00081249 -0.00109063 -0.00118667 -0.00164691]]]\n","■ dXt ----------------------------------\n"," [[[-0.00081249 -0.00118667]\n","  [-0.00309621 -0.00515657]\n","  [-0.00269259 -0.00691024]]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["array([[[-0.00081249, -0.00118667],\n","        [-0.00309621, -0.00515657],\n","        [-0.00269259, -0.00691024]]])"]},"metadata":{"tags":[]},"execution_count":27}]}]}