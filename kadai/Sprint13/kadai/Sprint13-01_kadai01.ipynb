{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Sprint13-01_kadai01.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"machine_shape":"hm","authorship_tag":"ABX9TyPjyU0ZmVafu8PQmE9iFgGj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"AJJGdte_obko","colab_type":"text"},"source":["# Sprint ディープラーニングフレームワーク1"]},{"cell_type":"markdown","metadata":{"id":"Oo-YA7HTocIp","colab_type":"text"},"source":["## 1.このSprintについて\n","\n","### Sprintの目的\n","- フレームワークのコードを読めるようにする\n","- フレームワークを習得し続けられるようになる\n","- 理論を知っている範囲をフレームワークで動かす\n","\n","### どのように学ぶか\n","- TensorFLowのサンプルコードを元に、これまで扱ってきたデータセットを学習していきます。"]},{"cell_type":"markdown","metadata":{"id":"6MMCYBnDocLa","colab_type":"text"},"source":["## 2.コードリーディング\n","\n","TensorFLowによって2値分類を行うサンプルコードを載せました。今回はこれをベースにして進めます。\n","\n","\n","tf.kerasやtf.estimatorなどの高レベルAPIは使用していません。低レベルなところから見ていくことにします。"]},{"cell_type":"markdown","metadata":{"id":"9miyS40p9vju","colab_type":"text"},"source":["## ライブラリインポート"]},{"cell_type":"code","metadata":{"id":"ml5OlmBX9y2h","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":51},"executionInfo":{"status":"ok","timestamp":1599790795468,"user_tz":-540,"elapsed":1069,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"3f89edce-09c7-4586-f27f-6085ad8c7cb1"},"source":["# ライブラリインポート\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import seaborn as sns\n","from sklearn import metrics \n","# from keras.datasets import mnist\n","from sklearn.model_selection import train_test_split\n","from sklearn.datasets import load_digits\n","from sklearn.preprocessing import OneHotEncoder\n","import pickle\n","%matplotlib inline"],"execution_count":1,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n","  import pandas.util.testing as tm\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"id":"HmxOzba_ocOw","colab_type":"text"},"source":["## 【問題1】スクラッチを振り返る\n","ここまでのスクラッチを振り返り、ディープラーニングを実装するためにはどのようなものが必要だったかを列挙してください。\n","\n","\n","（例）\n","\n","\n","- 重みを初期化する必要があった\n","- エポックのループが必要だった\n","\n","それらがフレームワークにおいてはどのように実装されるかを今回覚えていきましょう。"]},{"cell_type":"markdown","metadata":{"id":"NV1juiA22Ale","colab_type":"text"},"source":["## 【解答】\n","- w,b初期化\n","- 各層のレイヤークラスを呼び出しインスタンス生成\n","- 各層の活性化関数クラスを呼び出しインスタンス生成\n","- 損失関数のインスタンス生成\n","- 以下をエポック単位でループ\n","- 以下をデータセットをバッチ単位に変換してループ\n","- 各層の順伝播処理・活性化関数を通す処理を繰り返す\n","- 上記処理の計算結果を出力層に渡し、損失関数で損失計算する。\n","- 損失の微分計算をする\n","- 損失の微分値・最適化関数(SGD,AdaGradなど)を使用して損失関数が最小になるようw,bを更新"]},{"cell_type":"markdown","metadata":{"id":"yTsZ6u7tocSO","colab_type":"text"},"source":["### データセットの用意\n","以前から使用しているIrisデータセットを使用します。以下のサンプルコードではIris.csvが同じ階層にある想定です。\n","\n","<a href=\"https://www.kaggle.com/uciml/iris/data\">\n","Iris Species\n","</a>\n","\n","目的変数はSpeciesですが、3種類ある中から以下の2種類のみを取り出して使用します。\n","\n","```\n","Iris-versicolor\n","Iris-virginica\n","```"]},{"cell_type":"code","metadata":{"id":"6S-XsJxApmg_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599790795764,"user_tz":-540,"elapsed":1324,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"669cdf38-1dd3-48c5-8e25-81c767b5e639"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"O8CaOGr-rBe_","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599790796090,"user_tz":-540,"elapsed":1627,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"9eb0a4f2-5384-489e-cd3f-da18baeb1920"},"source":["ls \"/content/drive/My Drive/Colab Notebooks/dive_into_code/Sprint13/data/Iris.csv\""],"execution_count":3,"outputs":[{"output_type":"stream","text":["'/content/drive/My Drive/Colab Notebooks/dive_into_code/Sprint13/data/Iris.csv'\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"EQpGH1MJocU7","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"fjs10LH5ocXs","colab_type":"text"},"source":["## 【問題2】スクラッチとTensorFlowの対応を考える\n","以下のサンプルコードを見て、先ほど列挙した「ディープラーニングを実装するために必要なもの」がTensorFlowではどう実装されているかを確認してください。\n","\n","\n","それを簡単に言葉でまとめてください。単純な一対一の対応であるとは限りません。\n","\n","\n","《サンプルコード》\n","\n","\n","＊バージョン1.5から1.14の間で動作を確認済みです。\n","\n","```\n","\"\"\"\n","TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","# データセットの読み込み\n","dataset_path =\"Iris.csv\"\n","df = pd.read_csv(dataset_path)\n","# データフレームから条件抽出\n","df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n","y = df[\"Species\"]\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","y = np.array(y)\n","X = np.array(X)\n","# ラベルを数値に変換\n","y[y=='Iris-versicolor'] = 0\n","y[y=='Iris-virginica'] = 1\n","y = y.astype(np.int)[:, np.newaxis]\n","# trainとtestに分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n","\n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","# ハイパーパラメータの設定\n","learning_rate = 0.01\n","batch_size = 10\n","num_epochs = 10\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 1\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    return layer_output\n","# ネットワーク構造の読み込み                               \n","logits = example_net(X)\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# 推定結果\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","# variableの初期化\n","init = tf.global_variables_initializer()\n","\n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","            total_acc += acc\n","        total_loss /= n_samples\n","        total_acc /= n_samples\n","        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, loss, val_loss, acc, val_acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))\n","```"]},{"cell_type":"code","metadata":{"id":"GDt11LZpyVj5","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":255},"executionInfo":{"status":"ok","timestamp":1599790798629,"user_tz":-540,"elapsed":4146,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"bc702026-6ec3-4df2-f531-69b1fd678856"},"source":["pip install tensorflow==1.5"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow==1.5 in /usr/local/lib/python3.6/dist-packages (1.5.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.15.0)\n","Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (3.12.4)\n","Requirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.18.5)\n","Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (0.8.1)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (0.35.1)\n","Requirement already satisfied: tensorflow-tensorboard<1.6.0,>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.5) (1.5.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.4.0->tensorflow==1.5) (49.6.0)\n","Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (0.9999999)\n","Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (3.2.2)\n","Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (1.5.0)\n","Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (1.7.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorflow-tensorboard<1.6.0,>=1.5.0->tensorflow==1.5) (3.1.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GJbil_z5vrUh","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":428},"executionInfo":{"status":"ok","timestamp":1599790799844,"user_tz":-540,"elapsed":5336,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"c178b8e3-5d02-4e49-c4e7-63c157ae5071"},"source":["\"\"\"\n","TensorFlowで実装したニューラルネットワークを使いIrisデータセットを2値分類する\n","\"\"\"\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow as tf\n","# データセットの読み込み\n","dataset_path =\"/content/drive/My Drive/Colab Notebooks/dive_into_code/Sprint13/data/Iris.csv\"\n","df = pd.read_csv(dataset_path)\n","# データフレームから条件抽出\n","df = df[(df[\"Species\"] == \"Iris-versicolor\")|(df[\"Species\"] == \"Iris-virginica\")]\n","y = df[\"Species\"]\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","y = np.array(y)\n","X = np.array(X)\n","# ラベルを数値に変換\n","y[y=='Iris-versicolor'] = 0\n","y[y=='Iris-virginica'] = 1\n","y = y.astype(np.int)[:, np.newaxis]\n","# trainとtestに分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n","class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n"," \n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","# ハイパーパラメータの設定\n","learning_rate = 0.01\n","batch_size = 10\n","num_epochs = 10\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 1\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(\"float\", [None, n_input])\n","Y = tf.placeholder(\"float\", [None, n_classes])\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    return layer_output\n","# ネットワーク構造の読み込み\n","logits = example_net(X)\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# 推定結果\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","# variableの初期化\n","init = tf.global_variables_initializer()\n"," \n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","            total_acc += acc\n","        total_loss /= batch_size\n","        total_acc /= batch_size\n","        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:493: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:494: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:495: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:496: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:497: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:502: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 0, loss : 13.3832, val_loss : 8.3200, acc : 0.315, val_acc : 0.500\n","Epoch 1, loss : 1.0088, val_loss : 8.6625, acc : 0.575, val_acc : 0.688\n","Epoch 2, loss : 2.1459, val_loss : 5.1032, acc : 0.550, val_acc : 0.500\n","Epoch 3, loss : 0.4850, val_loss : 7.0238, acc : 0.575, val_acc : 0.750\n","Epoch 4, loss : 2.0544, val_loss : 6.9755, acc : 0.560, val_acc : 0.500\n","Epoch 5, loss : 1.9551, val_loss : 2.9305, acc : 0.590, val_acc : 0.875\n","Epoch 6, loss : 1.0904, val_loss : 1.1196, acc : 0.620, val_acc : 0.875\n","Epoch 7, loss : 0.6545, val_loss : 1.2856, acc : 0.640, val_acc : 0.875\n","Epoch 8, loss : 0.1458, val_loss : 1.4390, acc : 0.670, val_acc : 0.875\n","Epoch 9, loss : 0.1299, val_loss : 1.0209, acc : 0.670, val_acc : 0.875\n","test_acc : 0.900\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"2WuEgNmR5Tlt","colab_type":"text"},"source":["## 【解答】\n","- w,b初期化\n","```\n","82-91|\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","```\n","- 各層のレイヤークラスを呼び出しインスタンス生成\n","- 各層の活性化関数クラスを呼び出しインスタンス生成\n","  - 上記処理を関数にまとめて呼び出す記述が下記。<br>※プレースホルダーではテンソル要素のデータ型やテンソルの次元数を設定。<br>\n","  ```\n","  69 | n_input = X_train.shape[1]\n","  73 | X = tf.placeholder(\"float\", [None, n_input])\n","  98 | # ネットワーク構造の読み込み\n","  99 | logits = example_net(X)\n","  ```\n","- 損失関数のインスタンス生成\n","```\n","100-104 |\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","```\n","- 以下をエポック単位でループ\n","```\n","115 | for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","```\n","- 以下をデータセットをバッチ単位に変換してループ\n","```\n","120 | for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","```\n","- 各層の順伝播処理・活性化関数を通す処理を繰り返す\n","```\n","122 | sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","```\n","- 上記処理の計算結果を出力層に渡し、損失関数で損失計算する。\n","```\n","123 | loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","```\n","- 損失の微分計算をする\n","- 損失の微分値・最適化関数(SGD,AdaGradなど)を使用して損失関数が最小になるようw,bを更新"]},{"cell_type":"markdown","metadata":{"id":"5ZKSYQ_IYcmP","colab_type":"text"},"source":["### 【参考】\n","- <a href=\"https://www.atmarkit.co.jp/ait/articles/1804/18/news142.html\" style=\"text-decoration:none\">\n","   データフローグラフについて\n","</a>\n","\n","- <a href=\"https://www.atmarkit.co.jp/ait/articles/1804/20/news131.html\" style=\"text-decoration:none\">\n","    TensorFlowの基本構成要素：「テンソル」と「セッション」\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"heP956tVocda","colab_type":"text"},"source":["## 3.他のデータセットへの適用\n","\n","これまで扱ってきた小さなデータセットがいくつかあります。上記サンプルコードを書き換え、これらに対して学習・推定を行うニューラルネットワークを作成してください。\n","\n","\n","- Iris（3種類全ての目的変数を使用）\n","- House Prices\n","\n","どのデータセットもtrain, val, testの3種類に分けて使用してください。"]},{"cell_type":"markdown","metadata":{"id":"JD0oDDWFocgm","colab_type":"text"},"source":["## 【問題3】3種類全ての目的変数を使用したIrisのモデルを作成\n","Irisデータセットのtrain.csvの中で、目的変数Speciesに含まれる3種類全てを分類できるモデルを作成してください。\n","\n","\n","Iris Species\n","\n","\n","2クラスの分類と3クラス以上の分類の違いを考慮してください。それがTensorFlowでどのように書き換えられるかを公式ドキュメントなどを参考に調べてください。\n","\n","\n","《ヒント》\n","\n","\n","以下の2箇所は2クラス分類特有の処理です。\n","\n","```\n","loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","```\n","\n","```\n","correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","```\n","\n","メソッドは以下のように公式ドキュメントを確認してください。\n","\n","- <a href=\"https://www.tensorflow.org/api_docs/python/tf/nn/sigmoid_cross_entropy_with_logits\">\n","tf.nn.sigmoid_cross_entropy_with_logits  |  TensorFlow\n","</a>\n","\n","- <a href=\"https://www.tensorflow.org/api_docs/python/tf/math/sign\">\n","tf.math.sign  |  TensorFlow\n","</a>\n","\n","＊tf.signとtf.math.signは同じ働きをします。"]},{"cell_type":"markdown","metadata":{"id":"oznY2hshp2Jb","colab_type":"text"},"source":["## Iris（3種類全ての目的変数を使用）"]},{"cell_type":"code","metadata":{"id":"Qw2u8dtRqEAV","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1599790799846,"user_tz":-540,"elapsed":5312,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"fdccf5a6-88ad-4b5e-d0ae-52c6ffd5c00f"},"source":["# データセットの読み込み\n","dataset_path =\"/content/drive/My Drive/Colab Notebooks/dive_into_code/Sprint13/data/Iris.csv\"\n","df = pd.read_csv(dataset_path)\n","# データフレームから条件抽出\n","y = df[\"Species\"]\n","X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n","y = np.array(y)\n","X = np.array(X)\n","# ラベルを数値に変換\n","y[y=='Iris-versicolor'] = 0\n","y[y=='Iris-virginica'] = 1\n","y[y=='Iris-setosa'] = 2\n","y = y.astype(int)[:, np.newaxis]\n","en = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","y = en.fit_transform(y[:,:])\n","print(y[:10,:])\n","X.shape, y.shape"],"execution_count":6,"outputs":[{"output_type":"stream","text":["[[0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]\n"," [0. 0. 1.]]\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["((150, 4), (150, 3))"]},"metadata":{"tags":[]},"execution_count":6}]},{"cell_type":"code","metadata":{"id":"YPyDfx8Jq4cj","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599790799849,"user_tz":-540,"elapsed":5294,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}}},"source":["# trainとtestに分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"sfb46rtfpvhC","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1599790800143,"user_tz":-540,"elapsed":5575,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"36102c90-332d-474b-81ad-ffeb86090b06"},"source":["# ハイパーパラメータの設定\n","learning_rate = 0.01\n","batch_size = 10\n","num_epochs = 10\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 3\n","\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(tf.float32, [None, n_input])\n","Y = tf.placeholder(tf.int32, [None, n_classes])\n","# Y = tf.one_hot(Y, depth=n_classes)\n","\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    # layer_output = tf.nn.softmax(layer_output)\n","    return layer_output\n","# ネットワーク構造の読み込み\n","logits = example_net(X)\n","# 目的関数\n","# loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# 推定結果\n","# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","correct_pred = tf.equal(tf.argmax(Y,1),tf.argmax(tf.nn.softmax(logits),1))\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","# variableの初期化\n","init = tf.global_variables_initializer()\n"," \n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","            total_acc += acc\n","        total_loss /= batch_size\n","        total_acc /= batch_size\n","        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Epoch 0, loss : 57.5075, val_loss : 45.6710, acc : 0.353, val_acc : 0.625\n","Epoch 1, loss : 27.9084, val_loss : 24.1232, acc : 0.613, val_acc : 0.708\n","Epoch 2, loss : 8.4003, val_loss : 3.3415, acc : 0.717, val_acc : 0.792\n","Epoch 3, loss : 2.6964, val_loss : 1.7447, acc : 0.803, val_acc : 0.708\n","Epoch 4, loss : 1.6829, val_loss : 1.6309, acc : 0.890, val_acc : 0.917\n","Epoch 5, loss : 0.8191, val_loss : 1.6254, acc : 0.900, val_acc : 0.917\n","Epoch 6, loss : 0.4216, val_loss : 1.3666, acc : 0.950, val_acc : 0.917\n","Epoch 7, loss : 0.3091, val_loss : 1.2792, acc : 0.960, val_acc : 0.917\n","Epoch 8, loss : 0.2796, val_loss : 1.5205, acc : 0.950, val_acc : 0.917\n","Epoch 9, loss : 0.3092, val_loss : 1.9000, acc : 0.950, val_acc : 0.917\n","test_acc : 0.967\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"C1oXJN9-ockL","colab_type":"text"},"source":["## 【問題4】House Pricesのモデルを作成\n","回帰問題のデータセットであるHouse Pricesを使用したモデルを作成してください。\n","\n","\n","House Prices: Advanced Regression Techniques\n","\n","\n","この中のtrain.csvをダウンロードし、目的変数としてSalePrice、説明変数として、GrLivAreaとYearBuiltを使ってください。説明変数はさらに増やしても構いません。\n","\n","\n","分類問題と回帰問題の違いを考慮してください。"]},{"cell_type":"code","metadata":{"id":"S0MKt0R180eS","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599790800145,"user_tz":-540,"elapsed":5556,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"fccc579a-f867-4ca3-e8a1-aeb49a61d2c5"},"source":["df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/dive_into_code/Sprint13/data/train.csv')\n","X = np.array(df[['GrLivArea', 'YearBuilt']].copy())\n","y = np.array(df[['SalePrice']])\n","X.shape, y.shape"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((1460, 2), (1460, 1))"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"HEy4nIozCvnL","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599790800146,"user_tz":-540,"elapsed":5536,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}}},"source":["# trainとtestに分割\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"fvUydxhuCvwD","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":527},"executionInfo":{"status":"ok","timestamp":1599790801784,"user_tz":-540,"elapsed":7161,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"fde94696-795d-45a2-8187-21802fe0c858"},"source":["# ハイパーパラメータの設定\n","learning_rate = 0.01\n","batch_size = 20\n","num_epochs = 30\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 1\n","\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(tf.float32, [None, n_input])\n","Y = tf.placeholder(tf.float32, [None, n_classes])\n","\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    return layer_output\n","# ネットワーク構造の読み込み\n","logits = example_net(X)\n","# 目的関数\n","loss_op = tf.reduce_mean(tf.squared_difference(Y, logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# # 推定結果\n","# correct_pred = tf.equal(Y, logits)\n","# # 指標値計算\n","# accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","# variableの初期化\n","init = tf.global_variables_initializer()\n"," \n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n","        total_loss = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss = sess.run(loss_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","        total_loss /= batch_size\n","        val_loss = sess.run(loss_op, feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}\".format(epoch, total_loss, val_loss))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Epoch 0, loss : 8522314393.6000, val_loss : 2748112640.0000\n","Epoch 1, loss : 7643580796.8000, val_loss : 2804223232.0000\n","Epoch 2, loss : 7729416688.0000, val_loss : 2647673600.0000\n","Epoch 3, loss : 7695539894.4000, val_loss : 2512758016.0000\n","Epoch 4, loss : 7570357062.4000, val_loss : 2491301632.0000\n","Epoch 5, loss : 7520462304.0000, val_loss : 2487412224.0000\n","Epoch 6, loss : 7501029363.2000, val_loss : 2476545792.0000\n","Epoch 7, loss : 7475866915.2000, val_loss : 2470953472.0000\n","Epoch 8, loss : 7454729648.0000, val_loss : 2464623360.0000\n","Epoch 9, loss : 7437914563.2000, val_loss : 2468194816.0000\n","Epoch 10, loss : 7427024153.6000, val_loss : 2469970688.0000\n","Epoch 11, loss : 7420713584.0000, val_loss : 2465486336.0000\n","Epoch 12, loss : 7405587296.0000, val_loss : 2466824960.0000\n","Epoch 13, loss : 7385060025.6000, val_loss : 2487511296.0000\n","Epoch 14, loss : 7452801923.2000, val_loss : 2459700480.0000\n","Epoch 15, loss : 7379429216.0000, val_loss : 2468755968.0000\n","Epoch 16, loss : 7369007545.6000, val_loss : 2463557120.0000\n","Epoch 17, loss : 7365606281.6000, val_loss : 2466720256.0000\n","Epoch 18, loss : 7360216096.0000, val_loss : 2468666624.0000\n","Epoch 19, loss : 7356886390.4000, val_loss : 2466129408.0000\n","Epoch 20, loss : 7350019689.6000, val_loss : 2467399680.0000\n","Epoch 21, loss : 7347325974.4000, val_loss : 2469368832.0000\n","Epoch 22, loss : 7344708380.8000, val_loss : 2467490048.0000\n","Epoch 23, loss : 7337963520.0000, val_loss : 2481055488.0000\n","Epoch 24, loss : 7409808739.2000, val_loss : 2458466816.0000\n","Epoch 25, loss : 7345786592.0000, val_loss : 2465383168.0000\n","Epoch 26, loss : 7340515686.4000, val_loss : 2468899072.0000\n","Epoch 27, loss : 7333465129.6000, val_loss : 2473148160.0000\n","Epoch 28, loss : 7340855836.8000, val_loss : 2470829056.0000\n","Epoch 29, loss : 7330595865.6000, val_loss : 2475827968.0000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"mHw9nD1hocpw","colab_type":"text"},"source":["## 【問題5】MNISTのモデルを作成\n","ニューラルネットワークのスクラッチで使用したMNISTを分類するモデルを作成してください。\n","\n","\n","3クラス以上の分類という点ではひとつ前のIrisと同様です。入力が画像であるという点で異なります。\n","\n","\n","スクラッチで実装したモデルの再現を目指してください。"]},{"cell_type":"code","metadata":{"id":"fJ5oFOI2BvRI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599790801785,"user_tz":-540,"elapsed":7140,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}}},"source":["# # データ読み込み\n","# (X_train, y_train), (X_test, y_test) = mnist.load_data()\n","\n","# # 前処理\n","# X_train = X_train.astype(np.float)\n","# X_test = X_test.astype(np.float)\n","# X_train /= 255\n","# X_test /= 255"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NKnTjpTT8h6W","colab_type":"text"},"source":["TensorFlowのバージョンが合わず、上記コードが実行できなかったため、別ファイルで取得・出力したデータを読み込み。"]},{"cell_type":"code","metadata":{"id":"1_2YELAb5D6v","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599790801787,"user_tz":-540,"elapsed":7130,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}}},"source":["X_train_file = \"/content/drive/My Drive/Colab Notebooks/dive_into_code/Sprint13/data/X_train.csv\"\n","y_train_file = \"/content/drive/My Drive/Colab Notebooks/dive_into_code/Sprint13/data/y_train.csv\"\n","X_test_file = \"/content/drive/My Drive/Colab Notebooks/dive_into_code/Sprint13/data/X_test.csv\"\n","y_test_file = \"/content/drive/My Drive/Colab Notebooks/dive_into_code/Sprint13/data/y_test.csv\""],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"XwlMYMRk43Gz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"executionInfo":{"status":"ok","timestamp":1599790802693,"user_tz":-540,"elapsed":8019,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"c94f005f-539c-4e1d-9ab3-0d43f96cb0fb"},"source":["# read csv\n","with open(X_train_file, 'rb') as f1:\n","    X_train = pickle.load(f1)\n","with open(y_train_file, 'rb') as f2:\n","    y_train = pickle.load(f2)\n","with open(X_test_file, 'rb') as f3:\n","    X_test = pickle.load(f3)\n","with open(y_test_file, 'rb') as f4:\n","    y_test = pickle.load(f4)\n","\n","# shape\n","X_train.shape, y_train.shape, X_test.shape, y_test.shape"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((60000, 784), (60000,), (10000, 784), (10000,))"]},"metadata":{"tags":[]},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"PaEFuOAdBvAw","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599790802695,"user_tz":-540,"elapsed":7985,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}}},"source":["# # テスト実行用\n","# train_ln = 2000\n","# test_ln = 1000\n","\n","# X_train = X_train[:train_ln, :]\n","# y_train = y_train[:train_ln]\n","# X_test = X_test[:test_ln, :]\n","# y_test = y_test[:test_ln]\n","\n","# # shape\n","# X_train.shape, y_train.shape, X_test.shape, y_test.shape"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uhaz5tAo7chq","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":136},"executionInfo":{"status":"ok","timestamp":1599790802696,"user_tz":-540,"elapsed":7959,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"dfe96eb3-810a-4f7f-d544-76ae4cd5eb1a"},"source":["# one-hot\n","en = OneHotEncoder(handle_unknown='ignore', sparse=False)\n","y_train = en.fit_transform(y_train[:,np.newaxis])\n","y_test = en.transform(y_test[:,np.newaxis])\n","y_train"],"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0., 0., 0., ..., 0., 0., 0.],\n","       [1., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       ...,\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 0., 0.],\n","       [0., 0., 0., ..., 0., 1., 0.]])"]},"metadata":{"tags":[]},"execution_count":16}]},{"cell_type":"code","metadata":{"id":"BQzIxJ9N9BaU","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1599790802868,"user_tz":-540,"elapsed":8098,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}}},"source":["# さらにtrainとvalに分割\n","X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=0)"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"xXkd-FQyn9-k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":204},"executionInfo":{"status":"ok","timestamp":1599790836549,"user_tz":-540,"elapsed":41753,"user":{"displayName":"yoshimasa obata","photoUrl":"","userId":"05376502897348058331"}},"outputId":"d43e6132-49fe-4706-a19d-8ec085926abd"},"source":["class GetMiniBatch:\n","    \"\"\"\n","    ミニバッチを取得するイテレータ\n"," \n","    Parameters\n","    ----------\n","    X : 次の形のndarray, shape (n_samples, n_features)\n","      訓練データ\n","    y : 次の形のndarray, shape (n_samples, 1)\n","      正解値\n","    batch_size : int\n","      バッチサイズ\n","    seed : int\n","      NumPyの乱数のシード\n","    \"\"\"\n","    def __init__(self, X, y, batch_size = 10, seed=0):\n","        self.batch_size = batch_size\n","        np.random.seed(seed)\n","        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n","        self.X = X[shuffle_index]\n","        self.y = y[shuffle_index]\n","        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n","    def __len__(self):\n","        return self._stop\n","    def __getitem__(self,item):\n","        p0 = item*self.batch_size\n","        p1 = item*self.batch_size + self.batch_size\n","        return self.X[p0:p1], self.y[p0:p1]        \n","    def __iter__(self):\n","        self._counter = 0\n","        return self\n","    def __next__(self):\n","        if self._counter >= self._stop:\n","            raise StopIteration()\n","        p0 = self._counter*self.batch_size\n","        p1 = self._counter*self.batch_size + self.batch_size\n","        self._counter += 1\n","        return self.X[p0:p1], self.y[p0:p1]\n","# ハイパーパラメータの設定\n","learning_rate = 0.01\n","batch_size = 30\n","num_epochs = 10\n","n_hidden1 = 50\n","n_hidden2 = 100\n","n_input = X_train.shape[1]\n","n_samples = X_train.shape[0]\n","n_classes = 10\n","\n","# 計算グラフに渡す引数の形を決める\n","X = tf.placeholder(tf.float32, [None, n_input])\n","Y = tf.placeholder(tf.int32, [None, n_classes])\n","# Y = tf.one_hot(Y, depth=n_classes)\n","\n","# trainのミニバッチイテレータ\n","get_mini_batch_train = GetMiniBatch(X_train, y_train, batch_size=batch_size)\n","def example_net(x):\n","    \"\"\"\n","    単純な3層ニューラルネットワーク\n","    \"\"\"\n","    # 重みとバイアスの宣言\n","    weights = {\n","        'w1': tf.Variable(tf.random_normal([n_input, n_hidden1])),\n","        'w2': tf.Variable(tf.random_normal([n_hidden1, n_hidden2])),\n","        'w3': tf.Variable(tf.random_normal([n_hidden2, n_classes]))\n","    }\n","    biases = {\n","        'b1': tf.Variable(tf.random_normal([n_hidden1])),\n","        'b2': tf.Variable(tf.random_normal([n_hidden2])),\n","        'b3': tf.Variable(tf.random_normal([n_classes]))\n","    }\n","    layer_1 = tf.add(tf.matmul(x, weights['w1']), biases['b1'])\n","    layer_1 = tf.nn.relu(layer_1)\n","    layer_2 = tf.add(tf.matmul(layer_1, weights['w2']), biases['b2'])\n","    layer_2 = tf.nn.relu(layer_2)\n","    layer_output = tf.matmul(layer_2, weights['w3']) + biases['b3'] # tf.addと+は等価である\n","    # layer_output = tf.nn.softmax(layer_output)\n","    return layer_output\n","# ネットワーク構造の読み込み\n","logits = example_net(X)\n","# 目的関数\n","# loss_op = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(labels=Y, logits=logits))\n","loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=Y, logits=logits))\n","# 最適化手法\n","optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","train_op = optimizer.minimize(loss_op)\n","# 推定結果\n","# correct_pred = tf.equal(tf.sign(Y - 0.5), tf.sign(tf.sigmoid(logits) - 0.5))\n","correct_pred = tf.equal(tf.argmax(Y,1),tf.argmax(tf.nn.softmax(logits),1))\n","# 指標値計算\n","accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n","# variableの初期化\n","init = tf.global_variables_initializer()\n"," \n","# 計算グラフの実行\n","with tf.Session() as sess:\n","    sess.run(init)\n","    for epoch in range(num_epochs):\n","        # エポックごとにループ\n","        total_batch = np.ceil(X_train.shape[0]/batch_size).astype(np.int)\n","        total_loss = 0\n","        total_acc = 0\n","        for i, (mini_batch_x, mini_batch_y) in enumerate(get_mini_batch_train):\n","            # ミニバッチごとにループ\n","            sess.run(train_op, feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: mini_batch_x, Y: mini_batch_y})\n","            total_loss += loss\n","            total_acc += acc\n","        total_loss /= batch_size\n","        total_acc /= batch_size\n","        val_loss, val_acc = sess.run([loss_op, accuracy], feed_dict={X: X_val, Y: y_val})\n","        print(\"Epoch {}, loss : {:.4f}, val_loss : {:.4f}, acc : {:.3f}, val_acc : {:.3f}\".format(epoch, total_loss, val_loss, total_acc, val_acc))\n","    test_acc = sess.run(accuracy, feed_dict={X: X_test, Y: y_test})\n","    print(\"test_acc : {:.3f}\".format(test_acc))"],"execution_count":18,"outputs":[{"output_type":"stream","text":["Epoch 0, loss : 407.5536, val_loss : 1.1943, acc : 38.372, val_acc : 0.711\n","Epoch 1, loss : 46.7982, val_loss : 0.8797, acc : 41.498, val_acc : 0.813\n","Epoch 2, loss : 34.3635, val_loss : 0.7085, acc : 43.893, val_acc : 0.797\n","Epoch 3, loss : 29.4390, val_loss : 0.6538, acc : 45.237, val_acc : 0.831\n","Epoch 4, loss : 26.8385, val_loss : 0.5520, acc : 46.147, val_acc : 0.861\n","Epoch 5, loss : 22.5647, val_loss : 0.4607, acc : 47.021, val_acc : 0.887\n","Epoch 6, loss : 19.5731, val_loss : 0.4277, acc : 47.674, val_acc : 0.897\n","Epoch 7, loss : 16.6224, val_loss : 0.3337, acc : 48.600, val_acc : 0.915\n","Epoch 8, loss : 14.8828, val_loss : 0.3135, acc : 49.120, val_acc : 0.925\n","Epoch 9, loss : 12.8942, val_loss : 0.2921, acc : 49.610, val_acc : 0.928\n","test_acc : 0.930\n"],"name":"stdout"}]}]}